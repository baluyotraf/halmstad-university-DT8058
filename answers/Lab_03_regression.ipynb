{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name: Raffaello Baluyot\n",
    "## Course: DT8058"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1 style=\"font-size:40px;\">Regression</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the third lab in the Deep learning course! In this lab we will continue to take a look at four parts for MLP regression;\n",
    "* Introduction for setup and train an MLP\n",
    "* Model selection for classification\n",
    "* Impact of overfitting in validation performance \n",
    "* Avoid overfitting for a regression problem\n",
    "\n",
    "The lab includes different datasets, both synthetic and real for regression task. \n",
    "The first part of the lab uses two different synthetic regression problems. The **regr1()** synthetic dataset is a two-dimensional dataset with linear and non-linear relationships between the input features and the output value. It is a good benchmark dataset for regression models, as it is challenging and realistic. The **generate_piecewise_linear_data()** function generates a synthetic dataset with piecewise linear relationships between the input features and the output value with varying amount of noise for each piece. \n",
    "\n",
    "All **Tasks** include **TODO's** these are expected to be done before the deadline for this lab. The **Tasks** also include question(s), which should be answered and included in the report. Some sections do not contain any **TODO's** but are good to understand.\n",
    "\n",
    "Good luck!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regression task is to learn a function f that maps from a set of input features X to a continuous output value y. The input features X can be either real-valued or categorical. The output value y is a real-valued number.\n",
    "\n",
    "$$\n",
    "y = f(X) + \\epsilon\n",
    "$$\n",
    "\n",
    "The regression model is trained on a set of training data points, where the input features and the output values are known. The model learns to identify the relationship between the input features and the output values, and then uses this relationship to predict the output value for new input features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Necessary Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select gpu by index in case of multiple gpus\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "torch.manual_seed(0)\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's use GPU if possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set device\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPData:\n",
    "    \"\"\"\n",
    "    This class will manage all the dataset related functions for this lab.\n",
    "    Please take the time do go through the code and try to understand how each point is generated\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def regr1(N, v=0):\n",
    "        \"\"\"data(samples, features)\n",
    "\n",
    "        :param N: param v:  (Default value = 0)\n",
    "        :param v: Default value = 0)\n",
    "\n",
    "        \"\"\"\n",
    "        data = np.empty(shape=(N, 2), dtype=np.float64)\n",
    "\n",
    "        uni = lambda n: np.random.uniform(0, 1, n)\n",
    "        norm = lambda n: np.random.normal(0, 1, n)\n",
    "        noise = lambda n: np.random.normal(0, 1, (n,))\n",
    "        data[:, 0] = norm(N)\n",
    "        data[:, 1] = uni(N)\n",
    "\n",
    "        tar = 10 * data[:, 0] + np.sin(20 * np.pi * data[:, 1])\n",
    "        std_signal = np.std(tar)\n",
    "        no = noise(N)\n",
    "        tar = tar + v * std_signal * no\n",
    "        return data, tar\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_piecewise_linear_data(n_samples, n_segments, stocastic_noise=False):\n",
    "        \"\"\"\n",
    "        Generates a piecewise linear dataset with n_segments segments.\n",
    "        :param n_samples: Number of samples to generate\n",
    "        :param n_segments: Number of segments to use\n",
    "        :return: x, y\n",
    "\n",
    "        \"\"\"\n",
    "        x = torch.rand(n_samples) * 10  # Generate random input values between 0 and 10\n",
    "        y = torch.zeros(n_samples)\n",
    "\n",
    "        segment_length = 10 / n_segments\n",
    "        for i in range(n_segments):\n",
    "            mask = (x >= i * segment_length) & (x < (i + 1) * segment_length)\n",
    "            slope = torch.randn(1) * 2  # Random slope for each segment\n",
    "            if stocastic_noise:\n",
    "                noise = torch.randn(sum(mask)) * (0.5 + i * torch.randn(1))\n",
    "            else:\n",
    "                noise = torch.randn(sum(mask)) * (\n",
    "                    0.5 + i * 0.2\n",
    "                )  # Heteroscedastic noise\n",
    "            y[mask] = slope * x[mask] + noise\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do not forget to instanciate an object of the above class for you to be able to generate dataset on the fly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_datasets = MLPData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how each dataset looks like! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_distribution(imgs, shape=(2, 2)):\n",
    "    \"\"\"Plot scatter distribution for a list of images.\"\"\"\n",
    "    f, axs = plt.subplots(*shape, figsize=(10, 10))\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    if isinstance(imgs, list):\n",
    "        for idx, ((d, t), ax) in enumerate(zip(imgs, axs)):\n",
    "            ax.scatter(d[:, 0], d[:, 1], c=t)\n",
    "            ax.set_title(f\"Plot number: {idx}\")\n",
    "    elif isinstance(imgs, dict):\n",
    "        for (key, (d, t)), ax in zip(imgs.items(), axs):\n",
    "            ax.scatter(d[:, 0], d[:, 1], c=t)\n",
    "            ax.set_title(key)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting synthetic datasets\n",
    "# make sure you understand how the data is generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the synthetic dataset\n",
    "n_samples = 10000\n",
    "n_segments = 2\n",
    "x, y = synthetic_datasets.generate_piecewise_linear_data(\n",
    "    n_samples, n_segments, stocastic_noise=False\n",
    ")\n",
    "\n",
    "# Define colors for each segment\n",
    "segment_colors = [\n",
    "    \"b\",\n",
    "    \"g\",\n",
    "    \"r\",\n",
    "    \"c\",\n",
    "    \"m\",\n",
    "] * (\n",
    "    n_segments // 5 + 1\n",
    ")  # Repeat the colors to have enough colors for each segment (up to 5 segments\n",
    "\n",
    "# Plot the synthetic data with different colors for each segment\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i in range(n_segments):\n",
    "    mask = (x >= i * (10 / n_segments)) & (x < (i + 1) * (10 / n_segments))\n",
    "    plt.scatter(x[mask], y[mask], label=f\"Segment {i+1}\", c=segment_colors[i])\n",
    "\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.title(\"Piecewise Linear Regression with Heteroscedasticity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_distribution(\n",
    "    {\n",
    "        \"reg 0\": MLPData.regr1(1000, v=0),\n",
    "        \"reg 2\": MLPData.regr1(1000, v=2),\n",
    "        \"reg 5\": MLPData.regr1(1000, v=5),\n",
    "        \"reg 10\": MLPData.regr1(1000, v=10),\n",
    "    },\n",
    "    shape=(2, 2),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "\n",
    "**Model definition**\n",
    "\n",
    "In this lab exercise, you will design a Multi-Layer Perceptron (MLP) for regression. The goal is to create a simple neural network architecture to predict a continuous target variable based on input features. By completing this exercise, you will gain hands-on experience in configuring the architecture of an MLP for regression tasks.\n",
    "\n",
    "    Task Description:\n",
    "\n",
    "- Create an MLP architecture for regression.\n",
    "- Define the input layer, hidden layers, and output layer.\n",
    "- Configure the input layer to accept input data with dimensions specified as in_dimension.\n",
    "- Design the hidden layers with num_hidden_layers layers and hidden_nodes neurons in each layer.\n",
    "- Choose an appropriate activation function, specified as act, for the hidden layers. You can use common activation functions like ReLU (torch.nn.ReLU) for this purpose.\n",
    "- Configure the output layer to have a linear activation function since this is a regression task.\n",
    "- Define the output dimension to match your regression problem's requirements (specified as out_dimension).\n",
    "\n",
    "## Response\n",
    "\n",
    "See the implementation below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dimension=2,\n",
    "        hidden_nodes=1,\n",
    "        num_hidden_layers=1,\n",
    "        act=torch.nn.ReLU(),\n",
    "        out_dimension=1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        in_dimension: number of input data/features\n",
    "        hidden_nodes: number of neurons in the hidden layer(s)\n",
    "        num_hidden_layers: number of hidden layers\n",
    "        act: activation function\n",
    "        out_dimension: number of output neurons e.g. number of classes\n",
    "        \"\"\"\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.input = torch.nn.Linear(in_dimension, hidden_nodes)\n",
    "        self.hidden = [\n",
    "            torch.nn.Linear(hidden_nodes, hidden_nodes)\n",
    "            for _ in range(num_hidden_layers)\n",
    "        ]\n",
    "        self.output = torch.nn.Linear(hidden_nodes, out_dimension)\n",
    "        self.act = act\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input(x)\n",
    "        for h in self.hidden:\n",
    "            x = h(x)\n",
    "            x = self.act(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "    def predict(self, x):\n",
    "        return torch.argmax(self.forward(x), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the data for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to make tensor from the numpy data generated from ```MLPData``` class and use them to create a PyTorch dataset. For this exercise we will use ```TensorDataset```. To iterate over the dataset, we need a data loader. We will use the default ```DataLoader```. You can find the corresponding documentation [here](https://pytorch.org/docs/stable/data.html#torch.utils.data.TensorDataset) and [here](https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's start with 100 points\n",
    "x, y = synthetic_datasets.regr1(10000)\n",
    "# simply convert each array to a Tensor\n",
    "x = torch.Tensor(x)\n",
    "y = torch.Tensor(y.squeeze())\n",
    "# create the TensorDataset\n",
    "syn2_Pytorch = TensorDataset(x, y)\n",
    "# create the dataloader.\n",
    "loader = DataLoader(syn2_Pytorch, batch_size=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finish the below function. The task at this point is to create a function that is able to train your ```model``` for ```epoch_number``` using ```optimizer```, ```loss``` and ```dataloader```. You can read about optimizer [here](https://pytorch.org/docs/stable/optim.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Response\n",
    "\n",
    "See the implementation below.\n",
    "\n",
    "**Note:** the implementation assumes that the loss reduction function is \"mean\", since the function documentation says the return value is the average loss for the epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "        epoch: int, \n",
    "        optimizer:torch.optim.Optimizer, \n",
    "        loss: torch.nn.Module, \n",
    "        model: torch.nn.Module, \n",
    "        train_loader: DataLoader\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains the model for one epoch using the given training data.\n",
    "\n",
    "    Args:\n",
    "        epoch (int): The current epoch number.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer used for updating the model parameters.\n",
    "        loss (torch.nn.Module): The loss function used for calculating the loss.\n",
    "        model (torch.nn.Module): The neural network model to be trained.\n",
    "        train_loader (torch.utils.data.DataLoader): The data loader providing training data.\n",
    "\n",
    "    Returns:\n",
    "        float: The average loss for the epoch.\n",
    "\n",
    "    This function iterates through the provided `train_loader`, computes the forward pass,\n",
    "    calculates the loss, performs backpropagation, and updates the model parameters using\n",
    "    the given optimizer. It then returns the average loss for the entire epoch.\n",
    "\n",
    "    Example:\n",
    "        >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "        >>> loss_fn = torch.nn.MSELoss()\n",
    "        >>> train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "        >>> for epoch in range(num_epochs):\n",
    "        ...     epoch_loss = train_epoch(epoch, optimizer, loss_fn, model, train_loader)\n",
    "        ...     print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}\")\n",
    "    \"\"\"\n",
    "\n",
    "    total_loss = 0\n",
    "    total_items = 0\n",
    "    model.train(True)\n",
    "\n",
    "    for batch_idx, (xi, yi) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(xi)\n",
    "\n",
    "        loss_t = loss(outputs, yi.reshape(-1, 1))\n",
    "        loss_t.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        n_items = len(xi)\n",
    "        total_loss += loss_t.item() * n_items\n",
    "        total_items += n_items\n",
    "\n",
    "    return total_loss / total_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a way to trian our model, we need to create an instance of the model and train it. We still need a way to evaluate our model. In this simple datasets, we can try to visualize the decision boundaries. \n",
    "\n",
    "We will create one ```helper function```: ```plot_decision_boundary```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(dataset, y, model, steps=50):\n",
    "    xmin, xmax = dataset[:, 0].min(), dataset[:, 0].max()\n",
    "    ymin, ymax = dataset[:, 1].min(), dataset[:, 1].max()\n",
    "    x_span = np.linspace(xmin, xmax, steps)\n",
    "    y_span = np.linspace(ymin, ymax, steps)\n",
    "    xx_pred, yy_pred = np.meshgrid(x_span, y_span)\n",
    "    model_viz = np.array([xx_pred.flatten(), yy_pred.flatten()]).T\n",
    "\n",
    "    # Make predictions across region of interest\n",
    "    model.eval()\n",
    "    labels_predicted = model(Variable(torch.Tensor(model_viz)).float())\n",
    "\n",
    "    labels_predicted = labels_predicted.detach().numpy()\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(projection=\"3d\")\n",
    "    ax.scatter(dataset[:, 0], dataset[:, 1], y)\n",
    "    ax.scatter(\n",
    "        xx_pred.flatten(),\n",
    "        yy_pred.flatten(),\n",
    "        labels_predicted,\n",
    "        facecolor=(0, 0, 0, 0),\n",
    "        s=20,\n",
    "        edgecolor=\"#70b3f0\",\n",
    "    )\n",
    "    ax.view_init(elev=28, azim=120)\n",
    "    plt.show()\n",
    "    model.train()\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_reg(x, y, model):\n",
    "    \"\"\"\n",
    "    Returns the MSE and CorrCoef for a given dataset and y\n",
    "    \"\"\"\n",
    "\n",
    "    A = [\"MSE\", \"CorrCoeff\"]\n",
    "    model.eval()\n",
    "    preds = model(x)\n",
    "    pcorr = np.corrcoef(y.flatten(), preds.detach().numpy().flatten())\n",
    "    mse = torch.nn.MSELoss()(preds, y)\n",
    "\n",
    "    B = [mse.item(), pcorr]\n",
    "\n",
    "    print(f\"\\n {'#'*20} STATISTICS{'#'*20}\\n\")\n",
    "    for r in zip(A, B):\n",
    "        print(*r, sep=\"   \")\n",
    "    return print(f\"\\n {'#'*50}\")\n",
    "\n",
    "def stats_reg_ds(dataset: TensorDataset, model):\n",
    "    x = dataset.tensors[0]\n",
    "    y = dataset.tensors[1]\n",
    "    if y.ndim == 1:\n",
    "        y = y.reshape(-1, 1)\n",
    "    else:\n",
    "        raise ValueError()\n",
    "    return stats_reg(x, y, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiation\n",
    "Now the only thing missing to visualize you results is a trained network. **TODO:** Instantiate your model, loss and optimizer below. The choice of loss is critical for the training.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response\n",
    "\n",
    "See implementation below. 2 layers and 4 notes were selected. No particular reason but I think the size is small enough given that the problem is pretty trivial. Adding a higher than normal learning rate and some momentum for SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = NeuralNet(num_hidden_layers=2, hidden_nodes=4)\n",
    "critereon = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(my_model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "\n",
    "Now, our model, loss and optimizer are setup and we are ready to go Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = 100\n",
    "train_losses = list()\n",
    "for epoch in range(1, num_epoch + 1):\n",
    "    epoch_loss = train_epoch(epoch, optimizer, critereon, my_model, loader)\n",
    "    train_losses.append(epoch_loss)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{num_epoch}: Loss = {epoch_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(x, y, my_model, steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_reg(x, y.reshape(-1, 1), my_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the train losses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.figure().gca()\n",
    "plt.plot(np.arange(len(train_losses)), train_losses)\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss Value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection \n",
    "\n",
    "A proper training procedure is divided into ```3 splits```: training, validation and test. Generally, for each epoch, training is done on training data, and then a validation is done on the validation data. During validation the model weights are not updated. Best performing model on the validaiton data is selected and saved for final evaluation on test data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Split the data to 3 parts, one for training, one for validation and one hold out set for testing. A good starting point can be 70%, 15% and 15% of the dataset respectively for each split\n",
    "\n",
    "**HINT** you can either do this manually with indexing or use readily available tools e.g. in sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Response\n",
    "\n",
    "See implementation below. I picked the sklearn implementation so there's randomization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10000\n",
    "x, y = synthetic_datasets.regr1(n_samples)\n",
    "\n",
    "def split_dataset(x, y, train_size=0.7, valid_size=0.15, random_state=None, batch_size=1024):\n",
    "    others_size = 1.0 - train_size\n",
    "\n",
    "    train_x, others_x, train_y, others_y = train_test_split(x, y, train_size=train_size, random_state=random_state)\n",
    "    valid_x, test_x, valid_y, test_y = train_test_split(others_x, others_y, train_size=valid_size / others_size, random_state=0)\n",
    "\n",
    "    train_dataset = TensorDataset(torch.Tensor(train_x), torch.Tensor(train_y))\n",
    "    valid_dataset = TensorDataset(torch.Tensor(valid_x), torch.Tensor(valid_y))\n",
    "    test_dataset = TensorDataset(torch.Tensor(test_x), torch.Tensor(test_y))\n",
    "\n",
    "    return (\n",
    "        DataLoader(train_dataset, batch_size=batch_size),\n",
    "        DataLoader(valid_dataset, batch_size=batch_size),\n",
    "        DataLoader(test_dataset, batch_size=batch_size),\n",
    "    )\n",
    "\n",
    "train_loader, valid_loader, test_loader = split_dataset(x, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5\n",
    "**TODO:** Complete the following functions. Run a proper training on each of the synthetic datasets you have created. Discuss the performance of model in report. What could be the reason behind the performance? Feel free to adapt the number of hidden nodes (and possibly the number of hidden layers and epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response\n",
    "\n",
    "See implementation below. Similar notes as the training implementation since they are pretty similar.\n",
    "\n",
    "Result wise the performance of the model through the different datasets are not very different. This makes sense since the data is generated from a proper distribution, even if the sampling is performed with noise.\n",
    "\n",
    "The performance of the model is also very good, highly likely because the data I used didn't use the `v` parameter from the data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_epoch(epoch: int, loss: torch.nn.Module, model: torch.nn.Module, val_loader: DataLoader):\n",
    "    \"\"\"\n",
    "    Validates the model on the validation data for one epoch.\n",
    "\n",
    "    Args:\n",
    "        epoch (int): The current epoch number.\n",
    "        loss (torch.nn.Module): The loss function used for calculating the validation loss.\n",
    "        model (torch.nn.Module): The neural network model to be evaluated.\n",
    "        val_loader (torch.utils.data.DataLoader): The data loader providing validation data.\n",
    "\n",
    "    Returns:\n",
    "        float: The average validation loss for the epoch.\n",
    "\n",
    "    This function switches the provided model to evaluation mode, iterates through the\n",
    "    validation data provided by `val_loader`, computes the forward pass, and calculates\n",
    "    the validation loss. It then returns the average validation loss for the entire epoch.\n",
    "\n",
    "    Example:\n",
    "        >>> loss_fn = torch.nn.MSELoss()\n",
    "        >>> val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "        >>> for epoch in range(num_epochs):\n",
    "        ...     epoch_loss = validate_epoch(epoch, loss_fn, model, val_loader)\n",
    "        ...     print(f\"Epoch {epoch+1}, Validation Loss: {epoch_loss:.4f}\")\n",
    "    \"\"\"\n",
    "\n",
    "    total_loss = 0\n",
    "    total_items = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (xi, yi) in enumerate(val_loader):\n",
    "            output = model(xi)\n",
    "\n",
    "            n_items = len(xi)\n",
    "            total_loss += loss(output, yi.reshape(-1, 1)) * n_items\n",
    "            total_items += n_items\n",
    "\n",
    "    return total_loss / total_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a_proper_training(num_epoch, model, optimizer, loss, train_loader, val_loader):\n",
    "    \"\"\"\n",
    "    Performs a complete training and validation process for the given model.\n",
    "\n",
    "    Args:\n",
    "        num_epoch (int): The number of training epochs.\n",
    "        model (torch.nn.Module): The neural network model to be trained.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer used for updating the model parameters.\n",
    "        loss (torch.nn.Module): The loss function used for calculating the loss.\n",
    "        train_loader (torch.utils.data.DataLoader): The data loader providing training data.\n",
    "        val_loader (torch.utils.data.DataLoader): The data loader providing validation data.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the best trained model, a list of training losses for each epoch,\n",
    "        and a list of validation losses for each epoch.\n",
    "\n",
    "    This function trains the provided model for the specified number of epochs, monitoring both\n",
    "    training and validation losses. It also saves the best model based on the lowest validation\n",
    "    loss achieved during training.\n",
    "\n",
    "    Example:\n",
    "        >>> num_epochs = 10\n",
    "        >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "        >>> loss_fn = torch.nn.MSELoss()\n",
    "        >>> train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "        >>> val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "        >>> best_model, train_losses, val_losses = a_proper_training(num_epochs, model, optimizer, loss_fn, train_loader, val_loader)\n",
    "        >>> # After training, you can use the best_model for inference.\n",
    "    \"\"\"\n",
    "    best_val_loss = np.inf\n",
    "    best_model = None\n",
    "    train_losses = list()\n",
    "    val_losses = list()\n",
    "    for epoch in range(num_epoch):\n",
    "        train_loss = train_epoch(epoch, optimizer, loss, model, train_loader)\n",
    "        val_loss = validate_epoch(epoch, loss, model, val_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_model = copy.deepcopy(model)\n",
    "            best_val_loss = val_loss\n",
    "\n",
    "    return best_model, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = NeuralNet(num_hidden_layers=2, hidden_nodes=4)\n",
    "critereon = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(my_model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model, train_losses, val_losses = a_proper_training(\n",
    "    100, my_model, optimizer, critereon, train_loader, valid_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label=\"train\")\n",
    "plt.plot(val_losses, label=\"val\")\n",
    "plt.title(\"MSE Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_reg_ds(train_loader.dataset, best_model)\n",
    "stats_reg_ds(valid_loader.dataset, best_model)\n",
    "stats_reg_ds(test_loader.dataset, best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6 \n",
    "\n",
    "Add dropout to the model, and rerun the previous experiment, does it have any effect and why?\n",
    "\n",
    "## Response\n",
    "\n",
    "The dropout has the following effects:\n",
    "\n",
    "*   The training takes longer to converge: This makes sense since the connections are being dropped. Also, the generated dataset does not have feature redundancy as compared with real world data. While real world data can still utilize other features to makes sense of the labels, it's not the same for a equation generated, 2 input dataset.\n",
    "\n",
    "*   The training loss is worse than the validation loss: This is due to how dropouts work. It is applied in training, but not in test and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dimension=2,\n",
    "        hidden_nodes=1,\n",
    "        num_hidden_layers=1,\n",
    "        act=torch.nn.ReLU(),\n",
    "        out_dimension=1,\n",
    "        dropout_pct=0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        in_dimension: number of input data/features\n",
    "        hidden_nodes: number of neurons in the hidden layer(s)\n",
    "        num_hidden_layers: number of hidden layers\n",
    "        act: activation function\n",
    "        out_dimension: number of output neurons e.g. number of classes\n",
    "        \"\"\"\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.input = torch.nn.Linear(in_dimension, hidden_nodes)\n",
    "        self.hidden = [\n",
    "            torch.nn.Linear(hidden_nodes, hidden_nodes)\n",
    "            for _ in range(num_hidden_layers)\n",
    "        ]\n",
    "        self.output = torch.nn.Linear(hidden_nodes, out_dimension)\n",
    "        self.act = act\n",
    "        self.dropout = torch.nn.Dropout(p=dropout_pct)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input(x)\n",
    "        for h in self.hidden:\n",
    "            x = h(x)\n",
    "            x = self.act(x)\n",
    "            if self.dropout.p > 0:\n",
    "                x = self.dropout(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "    def predict(self, x):\n",
    "        return torch.argmax(self.forward(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10000\n",
    "x, y = synthetic_datasets.regr1(n_samples)\n",
    "\n",
    "train_loader, valid_loader, test_loader = split_dataset(x, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = NeuralNet(num_hidden_layers=2, hidden_nodes=4, dropout_pct=0.2)\n",
    "critereon = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(my_model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model, train_losses, val_losses = a_proper_training(\n",
    "    200, my_model, optimizer, critereon, train_loader, valid_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label=\"train\")\n",
    "plt.plot(val_losses, label=\"val\")\n",
    "plt.title(\"MSE Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_reg_ds(test_loader.dataset, best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7\n",
    "\n",
    "\n",
    "**TODO!** Rerun the experiment with the second synthetic dataset, with splitting, and proper training. Feel free to play with the parameters of the models\n",
    "\n",
    "## Response\n",
    "\n",
    "I had to increase the model complexity, due to the piece wise function being harder for the network to converge to compared to a continuous function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the synthetic dataset\n",
    "n_samples = 10000\n",
    "n_segments = 5\n",
    "x, y = synthetic_datasets.generate_piecewise_linear_data(\n",
    "    n_samples, n_segments, stocastic_noise=True\n",
    ")\n",
    "x = x.reshape(-1, 1)\n",
    "\n",
    "train_loader, valid_loader, test_loader = split_dataset(x, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = NeuralNet(in_dimension=1, num_hidden_layers=4, hidden_nodes=32)\n",
    "critereon = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(my_model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model, train_losses, val_losses = a_proper_training(\n",
    "    200, my_model, optimizer, critereon, train_loader, valid_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label=\"train\")\n",
    "plt.plot(val_losses, label=\"val\")\n",
    "plt.title(\"MSE Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_reg_ds(test_loader.dataset, best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1 style=\"font-size:40px;\">Real dataset</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following example, we will import the diabetes dataset. This dataset contains data from diabetic patients and contains certain features such as their bmi, age , blood pressure and glucose levels which are useful in predicting the diabetes disease progression in patients.\n",
    "\n",
    "Here are the key details of the sklearn diabetes dataset:\n",
    "\n",
    "- Target Variable: The target variable is a quantitative measure of disease progression, which represents the one-year change in a patient's disease progression. It is a continuous variable, not a binary classification of diabetes.\n",
    "- Features (Predictors):\n",
    "    The dataset contains ten baseline variables (predictors) that are used to predict the disease progression:\n",
    "\n",
    "        - Age\n",
    "        - Sex\n",
    "        - BMI (Body Mass Index)\n",
    "        - Average Blood Pressure\n",
    "        - S1: Total serum cholesterol\n",
    "        - S2: Low-density lipoproteins (LDL cholesterol)\n",
    "        - S3: High-density lipoproteins (HDL cholesterol)\n",
    "        - S4: Total cholesterol / HDL cholesterol ratio\n",
    "        - S5: Log of serum triglycerides level\n",
    "        - S6: Blood sugar level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = datasets.load_diabetes(scaled=True, as_frame=True, return_X_y=True)\n",
    "x, y = diabetes[0], diabetes[1]\n",
    "x = x.astype(np.float32)\n",
    "y = y.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(x.columns) + [\"Target\"]\n",
    "diabetes_df = pd.DataFrame(np.hstack([x, np.atleast_2d(y).T]), columns=cols)\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(diabetes_df.corr().values, vmin=-1, vmax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:**  run the experiments for the diabetes dataset, do you get similar performance? why? Do you suffer from overfitting/underfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response\n",
    "\n",
    "The performance I got is reasonable. The training loss is low and the validation loss is a bit higher. It's surprising that the test loss is lower but given the small number of data, I would assume that's just the variance of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader, test_loader = split_dataset(x.values, y.values, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = NeuralNet(in_dimension=x.shape[1], num_hidden_layers=8, hidden_nodes=256)\n",
    "critereon = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(my_model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model, train_losses, val_losses = a_proper_training(\n",
    "    10000, my_model, optimizer, critereon, train_loader, valid_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label=\"train\")\n",
    "plt.plot(val_losses, label=\"val\")\n",
    "plt.title(\"MSE Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_reg_ds(train_loader.dataset, my_model)\n",
    "stats_reg_ds(train_loader.dataset, best_model)\n",
    "stats_reg_ds(valid_loader.dataset, best_model)\n",
    "stats_reg_ds(test_loader.dataset, best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 10\n",
    "\n",
    "**TODO** try applying l1 and l2 regularization, does it help with the performance? Why?\n",
    "\n",
    "**HINT** in pytorch l2 can be applied by adding weight decay to optimizer, and for adding L1, you can choose HuberLoss instead of MSE. Or feel free to apply them manually\n",
    "\n",
    "## Response\n",
    "\n",
    "The only noticeable thing is that it takes longer for the validation loss to separate from the training loss in the graph. Otherwise the performance is more or less then same. I tried different values for the regularization values and they mostly just worsen the result.\n",
    "\n",
    "Also, I'm not sure if the `HuberLoss` is the right implementation of L1 regularization. It looks like a piecewise loss to me rather than an additional terms to the loss. But I will follow the hint for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = NeuralNet(in_dimension=x.shape[1], num_hidden_layers=8, hidden_nodes=256)\n",
    "critereon = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(my_model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.005)\n",
    "best_model, train_losses, val_losses = a_proper_training(\n",
    "    10000, my_model, optimizer, critereon, train_loader, valid_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label=\"train\")\n",
    "plt.plot(val_losses, label=\"val\")\n",
    "plt.title(\"MSE Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_reg_ds(train_loader.dataset, my_model)\n",
    "stats_reg_ds(train_loader.dataset, best_model)\n",
    "stats_reg_ds(valid_loader.dataset, best_model)\n",
    "stats_reg_ds(test_loader.dataset, best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = NeuralNet(in_dimension=x.shape[1], num_hidden_layers=8, hidden_nodes=256)\n",
    "critereon = torch.nn.HuberLoss(delta=200)\n",
    "optimizer = torch.optim.SGD(my_model.parameters(), lr=0.01, momentum=0.9)\n",
    "best_model, train_losses, val_losses = a_proper_training(\n",
    "    10000, my_model, optimizer, critereon, train_loader, valid_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label=\"train\")\n",
    "plt.plot(val_losses, label=\"val\")\n",
    "plt.title(\"Huber Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_reg_ds(train_loader.dataset, my_model)\n",
    "stats_reg_ds(train_loader.dataset, best_model)\n",
    "stats_reg_ds(valid_loader.dataset, best_model)\n",
    "stats_reg_ds(test_loader.dataset, best_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
