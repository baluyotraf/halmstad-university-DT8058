{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486e85e2-9b54-4254-8404-8f4efd41842b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from pathlib import Path\n",
    "import copy\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision\n",
    "import torchmetrics\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1528a9bc-b3bf-42df-b9aa-a1c6e1471520",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"/nfs/home/rafman23/jupyter/FlyingObjectDataset_10K\")\n",
    "training_dir = data_dir.joinpath(\"training\")\n",
    "validation_dir = data_dir.joinpath(\"validation\")\n",
    "testing_dir = data_dir.joinpath(\"testing\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "writer = torch.utils.tensorboard.SummaryWriter(\"runs/seg\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18477bd1-2b5a-4a06-820f-60a5b4875881",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationFlyingObjectsDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, root, transform):\n",
    "        super().__init__()\n",
    "        self.root = Path(root)\n",
    "        self.transform = transform\n",
    "        self.image_paths = sorted(self.root.joinpath(\"image\").glob(\"*\"))\n",
    "        self.seg_paths = [\n",
    "            self.root.joinpath(\"gt_image\", p.relative_to(self.root.joinpath(\"image\"))).with_name(f\"gt_{p.name}\")\n",
    "            for p in self.image_paths\n",
    "        ]\n",
    "\n",
    "    def _read_image(self, path):\n",
    "        return np.array(Image.open(path))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = self._read_image(self.image_paths[index])\n",
    "        seg = self._read_image(self.seg_paths[index])\n",
    "\n",
    "        transform = self.transform(image=image)\n",
    "\n",
    "        image_tr = transform[\"image\"]\n",
    "        seg_tr = self.transform.replay(transform[\"replay\"], image=seg)[\"image\"]\n",
    "        return image_tr, seg_tr\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff1d60a-2fdb-46d1-bb0e-ea51a8fe1836",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = A.ReplayCompose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.RandomResizedCrop(128, 128, scale=(0.95, 1.05), ratio=(1.0, 1.0), interpolation=cv2.INTER_LANCZOS4),\n",
    "    A.Normalize(mean=0.0, std=1.0),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "test_transform = A.ReplayCompose([\n",
    "    A.Normalize(mean=0.0, std=1.0),\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8811f87c-a4e7-4619-ad8e-089a9edfb6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SegmentationFlyingObjectsDataset(\n",
    "    training_dir,\n",
    "    train_transform\n",
    ")\n",
    "valid_dataset = SegmentationFlyingObjectsDataset(\n",
    "    validation_dir,\n",
    "    test_transform\n",
    ")\n",
    "test_dataset = SegmentationFlyingObjectsDataset(\n",
    "    testing_dir,\n",
    "    test_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6023a9db-e2e4-40e7-8443-41a9c80e1f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_image, sample_seg = zip(*[train_dataset[i] for i in range(100)])\n",
    "sample_image = torch.stack(sample_image)\n",
    "sample_seg = torch.stack(sample_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e407c03f-c317-4379-b2a4-e83ce9848cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.imshow(torchvision.utils.make_grid(sample_image, nrow=20).permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac29133-3272-40f1-8c39-34d014f93ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.imshow(torchvision.utils.make_grid(sample_seg, nrow=20).permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfff2c35-4516-4316-91d7-fdb2b281caa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_image_seg = torch.stack([sample_image, sample_seg], dim=1).view(-1, 3, 128, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30980445-5269-4063-ba21-fc868bdd8a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 50))\n",
    "plt.axis(\"off\")\n",
    "idx = np.random.choice(np.arange(len(x)), size=25, replace=False)\n",
    "plt.imshow(torchvision.utils.make_grid(sample_image_seg[:100], nrow=10, pad_value=0.5, padding=10).permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cfa8a5-69e4-4da7-a700-ea2b9b8a2727",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    dataset=valid_dataset,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e771a131-144d-414a-ba81-8492ef932d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetBlock(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_channels, output_channels, kernel_size=3, predown=False, postup=False):\n",
    "        super().__init__()\n",
    "        padding = (kernel_size - 1) // 2\n",
    "        prelayers = [torch.nn.MaxPool2d(2, 2)] if predown else []\n",
    "        postlayers = [torch.nn.ConvTranspose2d(output_channels, output_channels // 2, 2, 2)] if postup else []\n",
    "            \n",
    "        self.main = torch.nn.Sequential(\n",
    "            *prelayers,\n",
    "            torch.nn.Conv2d(input_channels, output_channels, kernel_size, 1, padding),\n",
    "            torch.nn.LeakyReLU(0.2),\n",
    "            torch.nn.BatchNorm2d(output_channels),\n",
    "            torch.nn.Conv2d(output_channels, output_channels, kernel_size, 1, padding),\n",
    "            torch.nn.LeakyReLU(0.2),\n",
    "            *postlayers\n",
    "        )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b56ea5f-b5e6-4373-ad73-43be584bb0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_features=64, n_outputs=3):\n",
    "        super().__init__()\n",
    "        self.downblock1 = UnetBlock(3, n_features)\n",
    "        self.downblock2 = UnetBlock(n_features, n_features * 2, predown=True)\n",
    "        self.downblock3 = UnetBlock(n_features * 2, n_features * 4, predown=True) \n",
    "        self.downblock4 = UnetBlock(n_features * 4, n_features * 8, predown=True) \n",
    "        self.downblock5 = UnetBlock(n_features * 8, n_features * 16, predown=True) \n",
    "        \n",
    "        self.bottom = UnetBlock(n_features * 16, n_features * 32, kernel_size=1, predown=True, postup=True)   \n",
    "\n",
    "        self.upblock5 = UnetBlock(n_features * 32, n_features * 16, postup=True)\n",
    "        self.upblock4 = UnetBlock(n_features * 16, n_features * 8, postup=True)\n",
    "        self.upblock3 = UnetBlock(n_features * 8, n_features * 4, postup=True)\n",
    "        self.upblock2 = UnetBlock(n_features * 4, n_features * 2, postup=True)\n",
    "        self.upblock1 = UnetBlock(n_features * 2, n_outputs)\n",
    "\n",
    "    def forward(self, input):\n",
    "        downblock1 = self.downblock1(input)\n",
    "        downblock2 = self.downblock2(downblock1)\n",
    "        downblock3 = self.downblock3(downblock2)\n",
    "        downblock4 = self.downblock4(downblock3)\n",
    "        downblock5 = self.downblock5(downblock4)\n",
    "        \n",
    "        bottom = self.bottom(downblock5)\n",
    "\n",
    "        upblock5 = self.upblock5(torch.cat([downblock5, bottom], dim=1))\n",
    "        upblock4 = self.upblock4(torch.cat([downblock4, upblock5], dim=1))\n",
    "        upblock3 = self.upblock3(torch.cat([downblock3, upblock4], dim=1))\n",
    "        upblock2 = self.upblock2(torch.cat([downblock2, upblock3], dim=1))\n",
    "        upblock1 = self.upblock1(torch.cat([downblock1, upblock2], dim=1))\n",
    "        return torch.nn.functional.sigmoid(upblock1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a48ba8e-4c8b-47f5-80ee-04be05872482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    epoch,\n",
    "    optimizer:torch.optim.Optimizer, \n",
    "    loss_fn: torch.nn.Module, \n",
    "    model: torch.nn.Module, \n",
    "    train_loader: torch.utils.data.DataLoader,\n",
    "    writer,\n",
    "):\n",
    "    total_loss = 0\n",
    "    total_items = 0\n",
    "    model.train(True)\n",
    "\n",
    "    for idx, (inputs, labels) in enumerate(tqdm(train_loader)):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        n_items = len(inputs)\n",
    "        total_loss += loss.item() * n_items\n",
    "        total_items += n_items\n",
    "\n",
    "        iteration_number = idx + epoch * len(train_loader)\n",
    "        writer.add_scalar(\"training_loss_step\", loss.item(), iteration_number)\n",
    "\n",
    "    return total_loss / total_items\n",
    "\n",
    "def validate_epoch(\n",
    "    loss_fn: torch.nn.Module, \n",
    "    model: torch.nn.Module, \n",
    "    val_loader: torch.utils.data.DataLoader\n",
    "):\n",
    "    total_loss = 0\n",
    "    total_items = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(val_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            n_items = len(inputs)\n",
    "            total_loss += loss_fn(outputs, labels).item() * n_items\n",
    "            total_items += n_items\n",
    "\n",
    "    return total_loss / total_items\n",
    "\n",
    "def training_loop(num_epoch, writer, model, optimizer, loss_fn, train_loader, val_loader, model_path):\n",
    "    best_val_loss = np.inf\n",
    "    best_model = None\n",
    "\n",
    "    train_losses = list()\n",
    "    val_losses = list()\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        train_loss = train_epoch(epoch, optimizer, loss_fn, model, train_loader, writer)\n",
    "        train_losses.append(train_loss)\n",
    "        writer.add_scalar(\"training_loss_epoch\", train_loss, epoch)\n",
    "        \n",
    "        val_loss = validate_epoch(loss_fn, model, val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        writer.add_scalar(\"validation_loss_epoch\", val_loss, epoch)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            torch.save(model, model_path)\n",
    "            best_model = copy.deepcopy(model)\n",
    "            best_val_loss = val_loss\n",
    "        print(f\"epoch {epoch + 1}: loss: {train_loss:0.4f} val loss: {val_loss:0.4f}\")\n",
    "\n",
    "    return best_model, train_losses, val_losses\n",
    "\n",
    "def predict(model: torch.nn.Module, test_loader: torch.utils.data.DataLoader):\n",
    "    with torch.no_grad():\n",
    "        true = []\n",
    "        pred = []\n",
    "        for inputs, labels in tqdm(test_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            true.append(labels.detach())\n",
    "            pred.append(outputs.detach())\n",
    "\n",
    "    return torch.cat(true).cpu(), torch.cat(pred).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270ccec5-386d-45d2-8bcb-f2f4defe8439",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = Unet().to(device)\n",
    "optimizer = torch.optim.Adam(unet.parameters(), lr=0.01)\n",
    "\n",
    "def dice_loss(pred, true, smooth=1.0):\n",
    "    intersection = (pred * true).sum(dim=[2,3])\n",
    "    union = pred.sum(dim=[2,3]) + true.sum(dim=[2,3])\n",
    "    coef = ((2.0 * intersection + smooth) / (union + smooth)).mean(dim=1)\n",
    "    loss = 1 - coef\n",
    "\n",
    "    return loss.mean()\n",
    "\n",
    "def calculate_loss(pred, true, bce_weight=0.5):\n",
    "    dice = dice_loss(pred, true)\n",
    "    bce = torch.nn.functional.binary_cross_entropy(pred, true)\n",
    "    return dice * (1 - bce_weight) + bce * bce_weight\n",
    "    return torch.nn.functional.binary_cross_entropy(pred, true)\n",
    "    \n",
    "torch.cuda.empty_cache()\n",
    "writer.add_graph(unet, sample_image.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d62c3da-fc85-4697-80d7-b40e9f128150",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model, training_loss, validation_loss = training_loop(200, writer, unet, optimizer, calculate_loss, train_loader, valid_loader, \"best_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca40db2c-c720-40e3-82df-d062a1e96876",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7.5))\n",
    "plt.plot(training_loss, label=\"training\")\n",
    "plt.plot(validation_loss, label=\"validation\")\n",
    "plt.title(\"0.5 Dice + 0.5 BCE Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a696cede-aa84-419b-bb24-2710c83b2a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = validate_epoch(calculate_loss, best_model, test_loader)\n",
    "test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43b4f2a-9cbd-4ba0-8725-b0a2ebe0d7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    metrics, \n",
    "    model: torch.nn.Module, \n",
    "    loader: torch.utils.data.DataLoader\n",
    "):\n",
    "    total_loss = dict(zip(metrics.keys(), [0]*len(metrics)))\n",
    "    total_items = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(loader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            n_items = len(inputs)\n",
    "            for m, fn in metrics.items():\n",
    "                total_loss[m] += fn(outputs, labels).item() * n_items \n",
    "            total_items += n_items\n",
    "\n",
    "    for m, fn in metrics.items():\n",
    "        total_loss[m] /= total_items\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef0714e-b189-4613-b33e-8472e5cb9f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    \"dice\": dice_loss,\n",
    "    \"bce\": torch.nn.functional.binary_cross_entropy,\n",
    "    \"mse\": torch.nn.functional.mse_loss,\n",
    "    \"acc\": lambda p, t: ((p > 0.5) == t).float().mean()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336ffb6f-9024-4f4d-b0d0-40dfe7d57aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eval_loader = torch.utils.data.DataLoader(\n",
    "    dataset=SegmentationFlyingObjectsDataset(\n",
    "        training_dir,\n",
    "        test_transform\n",
    "    ),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "evaluate(metrics, best_model, train_eval_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebf3450-c095-4d0b-9c38-c62fb898ef95",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(metrics, best_model, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81458385-c082-4fd1-9fd8-fb8303e57878",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(metrics, best_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9c7cba-c383-4c32-b5c0-f4fc77aef9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_true, test_pred = predict(best_model, test_loader)\n",
    "padding = torch.ones(*test_true.shape[:-1], 2)\n",
    "test_true_pred = torch.cat([test_pred, padding, test_true], dim=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bd8a20-2198-4dcf-b294-6b739fde0603",
   "metadata": {},
   "outputs": [],
   "source": [
    "error = ((test_true - test_pred) ** 2).sum(dim=[1,2,3])\n",
    "error_values, error_indices = error.topk(25)\n",
    "good_values, good_indices = (-error).topk(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecf647e-ec1b-425b-8de2-edc87f3f9a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 50))\n",
    "plt.title(\"Bad Results\")\n",
    "plt.axis(\"off\")\n",
    "idx = np.random.choice(np.arange(len(x)), size=25, replace=False)\n",
    "plt.imshow(torchvision.utils.make_grid(x[error_indices], nrow=5, pad_value=1.0, padding=10).permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb17b80-05d1-4f20-8244-acc1350e5f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 50))\n",
    "plt.title(\"Good Results\")\n",
    "plt.axis(\"off\")\n",
    "idx = np.random.choice(np.arange(len(x)), size=25, replace=False)\n",
    "plt.imshow(torchvision.utils.make_grid(x[good_indices[-25:]], nrow=5, pad_value=1.0, padding=10).permute(1, 2, 0))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
